{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "toc": {
      "nav_menu": {
        "height": "263px",
        "width": "352px"
      },
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "machine summarisation from scratch.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBnO6rAraljS",
        "outputId": "9bc21515-4830-4224-ac21-bc35825d92b1"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf \n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords #provides list of english stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "stop = stopwords.words('english')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "XXDgkf9waljh",
        "outputId": "2e081cd2-f85d-4516-ba2b-9ec597b537fd"
      },
      "source": [
        "#PRINT VERSION!!!\n",
        "tf.__version__"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.6.0'"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "SH2v06qjaljn"
      },
      "source": [
        "train = pd.read_csv('/content/drive/MyDrive/Datasets/Text_Summarization/Reviews.csv')\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "adlWCm93aljp",
        "outputId": "77981206-f27c-4244-e883-b792a72862d0"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>ProductId</th>\n",
              "      <th>UserId</th>\n",
              "      <th>ProfileName</th>\n",
              "      <th>HelpfulnessNumerator</th>\n",
              "      <th>HelpfulnessDenominator</th>\n",
              "      <th>Score</th>\n",
              "      <th>Time</th>\n",
              "      <th>Summary</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>B001E4KFG0</td>\n",
              "      <td>A3SGXH7AUHU8GW</td>\n",
              "      <td>delmartian</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1303862400</td>\n",
              "      <td>Good Quality Dog Food</td>\n",
              "      <td>I have bought several of the Vitality canned d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>B00813GRG4</td>\n",
              "      <td>A1D87F6ZCVE5NK</td>\n",
              "      <td>dll pa</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1346976000</td>\n",
              "      <td>Not as Advertised</td>\n",
              "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>B000LQOCH0</td>\n",
              "      <td>ABXLMWJIXXAIN</td>\n",
              "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1219017600</td>\n",
              "      <td>\"Delight\" says it all</td>\n",
              "      <td>This is a confection that has been around a fe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>B000UA0QIQ</td>\n",
              "      <td>A395BORC6FGVXV</td>\n",
              "      <td>Karl</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1307923200</td>\n",
              "      <td>Cough Medicine</td>\n",
              "      <td>If you are looking for the secret ingredient i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>B006K2ZZ7K</td>\n",
              "      <td>A1UQRSCLF8GW1T</td>\n",
              "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>1350777600</td>\n",
              "      <td>Great taffy</td>\n",
              "      <td>Great taffy at a great price.  There was a wid...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Id  ...                                               Text\n",
              "0   1  ...  I have bought several of the Vitality canned d...\n",
              "1   2  ...  Product arrived labeled as Jumbo Salted Peanut...\n",
              "2   3  ...  This is a confection that has been around a fe...\n",
              "3   4  ...  If you are looking for the secret ingredient i...\n",
              "4   5  ...  Great taffy at a great price.  There was a wid...\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxcpeiTqaljr"
      },
      "source": [
        "# We only need the last two columns for the text_summarization task\n",
        "train = train[['Summary','Text']]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaBoJB11aljt"
      },
      "source": [
        "# We create a new column to indicate the length of \n",
        "# the sentence\n",
        "train['text_length'] = train['Text'].str.count(' ')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sy66-wvlaljv",
        "outputId": "ecd3de8f-ee85-4c06-d417-3491b9f3bf44"
      },
      "source": [
        "train['text_length']"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0         48\n",
              "1         30\n",
              "2         98\n",
              "3         42\n",
              "4         29\n",
              "          ..\n",
              "568449    25\n",
              "568450    45\n",
              "568451    70\n",
              "568452    36\n",
              "568453    20\n",
              "Name: text_length, Length: 568454, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZraAb7daljy",
        "outputId": "1b3b610f-4112-4e30-b36a-852c744902c8"
      },
      "source": [
        "# Another column to see the lengths of respective summaries\n",
        "train['summary_length'] = train['Summary'].str.count(' ')\n",
        "train['summary_length']\n",
        "# Shows that we have a small summary length."
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0         3.0\n",
              "1         2.0\n",
              "2         3.0\n",
              "3         1.0\n",
              "4         1.0\n",
              "         ... \n",
              "568449    3.0\n",
              "568450    0.0\n",
              "568451    3.0\n",
              "568452    4.0\n",
              "568453    1.0\n",
              "Name: summary_length, Length: 568454, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2uIvy1Oalj7"
      },
      "source": [
        "# Filtering out and using rows with small text and summary lengths.\n",
        "train = train.loc[train['summary_length']<8]\n",
        "train = train.loc[train['text_length']<30]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKb3eRENalkA",
        "outputId": "6715317e-f600-49fa-d643-950deb3cf55e"
      },
      "source": [
        "print(train.shape)\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(109053, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "nmRUm7ckcZuj",
        "outputId": "66fcfdf0-e930-4be3-d5da-2055056c2b17"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Summary</th>\n",
              "      <th>Text</th>\n",
              "      <th>text_length</th>\n",
              "      <th>summary_length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Great taffy</td>\n",
              "      <td>Great taffy at a great price.  There was a wid...</td>\n",
              "      <td>29</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Wonderful, tasty taffy</td>\n",
              "      <td>This taffy is so good.  It is very soft and ch...</td>\n",
              "      <td>27</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Yay Barley</td>\n",
              "      <td>Right now I'm mostly just sprouting this so my...</td>\n",
              "      <td>25</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Healthy Dog Food</td>\n",
              "      <td>This is a very healthy dog food. Good for thei...</td>\n",
              "      <td>24</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>fresh and greasy!</td>\n",
              "      <td>good flavor! these came securely packed... the...</td>\n",
              "      <td>14</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   Summary  ... summary_length\n",
              "4              Great taffy  ...            1.0\n",
              "7   Wonderful, tasty taffy  ...            2.0\n",
              "8               Yay Barley  ...            1.0\n",
              "9         Healthy Dog Food  ...            2.0\n",
              "13       fresh and greasy!  ...            2.0\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tNVvLp1alkC"
      },
      "source": [
        "# Convert to lower case\n",
        "train['text_lower'] = train['Text'].str.lower()\n",
        "# Remove punctuation\n",
        "train['text_no_punctuation'] = train['text_lower'].str.replace('[^\\w\\s]','')\n",
        "\n",
        "# We will not remove stop-words because they are needed to understand\n",
        "# the sequence of words for building summary.\n",
        "# If need be remove stopwords using the following code:\n",
        "#train['english_no_stopwords'] = train['english_no_punctuation'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
        "#train[\"english_no_stopwords\"] = train[\"english_no_stopwords\"].fillna(\"fillna\")\n",
        "#train[\"english_no_stopwords\"] = train[\"english_no_stopwords\"] "
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tV-wwAHwalkE"
      },
      "source": [
        "# Convert summary to lower case.\n",
        "train['summary_lower'] = train[\"Summary\"].str.lower()\n",
        "\n",
        "# add start and end token to each summary\n",
        "train['summary_no_punctuation'] =  '_start_' + ' ' +train['summary_lower'].str.replace('[^\\w\\s]','')+ ' ' +'_end_'"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEdbmbJwdJ3u"
      },
      "source": [
        "### MODEL BUILDING:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoFCg8PWalkL"
      },
      "source": [
        "max_features1 = 5000\n",
        "maxlen1 = 30 # for text\n",
        "\n",
        "max_features2 = 5000\n",
        "maxlen2 = 8 #for summary"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CNrnX8malkM"
      },
      "source": [
        "# Tokenize the text\n",
        "tok1 = tf.keras.preprocessing.text.Tokenizer(num_words=max_features1) \n",
        "#fit to cleaned text\n",
        "tok1.fit_on_texts(list(train['text_no_punctuation'].astype(str))) \n",
        "tf_train_text =tok1.texts_to_sequences(list(train['text_no_punctuation'].astype(str)))\n",
        "#let's execute pad step\n",
        "tf_train_text =tf.keras.preprocessing.sequence.pad_sequences(tf_train_text, maxlen=maxlen1)  "
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8z8FDfvWalkP"
      },
      "source": [
        "# Tokenize the summaries\n",
        "tok2 = tf.keras.preprocessing.text.Tokenizer(num_words=max_features2, filters = '*') \n",
        "#fit to cleaned summaries\n",
        "tok2.fit_on_texts(list(train['summary_no_punctuation'].astype(str)))\n",
        "tf_train_summary = tok2.texts_to_sequences(list(train['summary_no_punctuation'].astype(str)))\n",
        "tf_train_summary = tf.keras.preprocessing.sequence.pad_sequences(tf_train_summary, maxlen=maxlen2, padding ='post') "
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOhAoDd9alkR"
      },
      "source": [
        "### Define Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sokx7zI0alkT",
        "outputId": "b22ff3f3-14d8-47e4-8af1-2372acb06f75"
      },
      "source": [
        "# creating new variable for vectorized summary.\n",
        "vectorized_summary = tf_train_summary\n",
        "# For Decoder Input, you don't need the last word as that is only for prediction\n",
        "# when we are training using Teacher Forcing.\n",
        "decoder_input_data = vectorized_summary[:, :-1]\n",
        "\n",
        "# Decoder Target Data Is Ahead By 1 Time Step From Decoder Input Data (Teacher Forcing)\n",
        "decoder_target_data = vectorized_summary[:, 1:]\n",
        "\n",
        "print(f'Shape of decoder input: {decoder_input_data.shape}')\n",
        "print(f'Shape of decoder target: {decoder_target_data.shape}')\n",
        "\n",
        "vectorized_text = tf_train_text\n",
        "# Encoder input is simply the body of the issue text\n",
        "encoder_input_data = vectorized_text\n",
        "doc_length = encoder_input_data.shape[1]\n",
        "print(f'Shape of encoder input: {encoder_input_data.shape}')\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of decoder input: (109053, 7)\n",
            "Shape of decoder target: (109053, 7)\n",
            "Shape of encoder input: (109053, 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHEXh4oDalkW"
      },
      "source": [
        "# define the vocab size\n",
        "# +1 is for unk tokens\n",
        "vocab_size_encoder = len(tok1.word_index) + 1\n",
        "vocab_size_decoder = len(tok2.word_index) + 1"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S09I9vfRfuL_"
      },
      "source": [
        "NOTE--> Why to add 1?\n",
        "Because Tokenizer.word_index is a python dictionary that contains token keys (string) and token ID values (integer), and where the first token ID is 1 (not zero) and where the token IDs are assigned incrementally.  Therefore, we need vocabulary of size len(word_index) + 1 to be able to index up to the greatest token ID."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBkmIykAalkY"
      },
      "source": [
        "#arbitrarly set dimension for embedding and # of hidden units\n",
        "latent_dim = 50\n",
        "# It is a hyper-parameter.\n",
        "\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ihu_Sz3gXBG"
      },
      "source": [
        "##### ENCODER ARCHITECTURE:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eB8vomcRalkZ"
      },
      "source": [
        "encoder_inputs = tf.keras.Input(shape=(doc_length,), name='Encoder-Input')\n",
        "\n",
        "# Word embeding layer for encoder (English text)\n",
        "x = tf.keras.layers.Embedding(vocab_size_encoder, latent_dim, name='Body-Word-Embedding', mask_zero=False)(encoder_inputs)\n",
        "\n",
        "\n",
        "#Batch normalization layer is used so that the distribution of the inputs \n",
        "#to a specific layer doesn't change over time\n",
        "x = tf.keras.layers.BatchNormalization(name='Encoder-Batchnorm-1')(x)\n",
        "\n",
        "\n",
        "# We do not need the `encoder_output` just the hidden state.\n",
        "_, state_h = tf.keras.layers.GRU(latent_dim, return_state=True, name='Encoder-Last-GRU')(x)\n",
        "\n",
        "# Encapsulate the encoder as a separate entity so we can just \n",
        "#  encode without decoding if we want to.\n",
        "encoder_model = tf.keras.Model(inputs=encoder_inputs, outputs=state_h, name='Encoder-Model')\n",
        "\n",
        "seq2seq_encoder_out = encoder_model(encoder_inputs)\n",
        "\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOWk0_GTggLP"
      },
      "source": [
        "##### DECODER ARCHITECTURE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiluSrf6gkFL"
      },
      "source": [
        "# for teacher forcing\n",
        "decoder_inputs = tf.keras.Input(shape=(None,), name='Decoder-Input')\n",
        "\n",
        "# Word Embedding For Decoder (Italian text)\n",
        "dec_emb = tf.keras.layers.Embedding(vocab_size_decoder, latent_dim, name='Decoder-Word-Embedding', mask_zero=False)(decoder_inputs)\n",
        "#again batch normalization\n",
        "dec_bn = tf.keras.layers.BatchNormalization(name='Decoder-Batchnorm-1')(dec_emb)\n",
        "\n",
        "# Set up the decoder, using `decoder_state_input` as initial state.\n",
        "decoder_gru = tf.keras.layers.GRU(latent_dim, return_state=True, return_sequences=True, name='Decoder-GRU')\n",
        "decoder_gru_output, _ = decoder_gru(dec_bn, initial_state=seq2seq_encoder_out) #the decoder \"decodes\" the encoder output.\n",
        "x = tf.keras.layers.BatchNormalization(name='Decoder-Batchnorm-2')(decoder_gru_output)\n",
        "\n",
        "# Dense layer for prediction\n",
        "decoder_dense = tf.keras.layers.Dense(vocab_size_decoder, activation='softmax', name='Final-Output-Dense')\n",
        "decoder_outputs = decoder_dense(x)\n",
        "\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WR-ODsVLguUy"
      },
      "source": [
        "##### BUILDING A SEQ2SEQ MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYKcYdkIgtYY"
      },
      "source": [
        "seq2seq_Model = tf.keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "seq2seq_Model.compile(optimizer=tf.keras.optimizers.Nadam(learning_rate=0.001), loss='sparse_categorical_crossentropy')"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3k-ASY3alkd"
      },
      "source": [
        "** Examine Model Architecture Summary **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KI9zESZXalkd",
        "outputId": "d6cff004-cfd5-456e-8fcb-2e8236296a73"
      },
      "source": [
        "#from seq2seq_utils import viz_model_architecture\n",
        "seq2seq_Model.summary()\n",
        "#viz_model_architecture(seq2seq_Model)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Decoder-Input (InputLayer)      [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-Word-Embedding (Embeddi (None, None, 50)     692150      Decoder-Input[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-Input (InputLayer)      [(None, 30)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-Batchnorm-1 (BatchNorma (None, None, 50)     200         Decoder-Word-Embedding[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-Model (Functional)      (None, 50)           2013300     Encoder-Input[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-GRU (GRU)               [(None, None, 50), ( 15300       Decoder-Batchnorm-1[0][0]        \n",
            "                                                                 Encoder-Model[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-Batchnorm-2 (BatchNorma (None, None, 50)     200         Decoder-GRU[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "Final-Output-Dense (Dense)      (None, None, 13843)  705993      Decoder-Batchnorm-2[0][0]        \n",
            "==================================================================================================\n",
            "Total params: 3,427,143\n",
            "Trainable params: 3,426,843\n",
            "Non-trainable params: 300\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GL2Z6RjJalki"
      },
      "source": [
        "# Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1ajj5xDalkj",
        "outputId": "648f8779-cbbf-4110-9409-4d3b24eda46f"
      },
      "source": [
        "batch_size = 64\n",
        "epochs = 2 \n",
        "history = seq2seq_Model.fit([encoder_input_data, decoder_input_data], np.expand_dims(decoder_target_data, -1),\n",
        "          batch_size=batch_size,  epochs=epochs ,  validation_split=0.12) "
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "1500/1500 [==============================] - 271s 177ms/step - loss: 2.6436 - val_loss: 1.9662\n",
            "Epoch 2/2\n",
            "1500/1500 [==============================] - 266s 178ms/step - loss: 1.7893 - val_loss: 1.7473\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1qPD21halkk"
      },
      "source": [
        "#test_text = ['apparently they used too much synthetic flavors that it just burns your tongue also theres too much oil  almost made me chok']\n",
        "#test_text = ['this stuff is awesome  for best flavor boil it in water drain the water add spice packet and then add hot water']\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeeicWt9jRof"
      },
      "source": [
        "print(train['text_no_punctuation'][568442])\n",
        "test_text = ['this product is great  gives you so much energy and tastes great  try this cafe latte and all the other flavors and you will not be disappointed']"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyD5qhkWalkl"
      },
      "source": [
        "#seq2seq_Model = tf.keras.models.load_model('seq2seq_subsample_1_epochs.h5')\n",
        "#seq2seq_Model = tf.keras.models.load_model('seq2seq_full_data_3_epochs.h5')"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eX3k3XKvalkm"
      },
      "source": [
        "#max_len_title = 30\n",
        "# get the encoder's features for the decoder\n",
        "tok1.fit_on_texts(test_text)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QCrjj48alkn"
      },
      "source": [
        "raw_tokenized = tok1.texts_to_sequences(test_text)\n",
        "raw_tokenized = tf.keras.preprocessing.sequence.pad_sequences(raw_tokenized, maxlen=maxlen1)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mssf4qYWalko"
      },
      "source": [
        "body_encoding = encoder_model.predict(raw_tokenized) #predict the encoder state of the new sentence"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDNOPwRdalkp"
      },
      "source": [
        "latent_dim = seq2seq_Model.get_layer('Decoder-Word-Embedding').output_shape[-1]"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_aOihgLalkq"
      },
      "source": [
        "#remember the get layer method for getting the embedding (word clusters)\n",
        "decoder_inputs = seq2seq_Model.get_layer('Decoder-Input').input \n",
        "dec_emb = seq2seq_Model.get_layer('Decoder-Word-Embedding')(decoder_inputs)\n",
        "dec_bn = seq2seq_Model.get_layer('Decoder-Batchnorm-1')(dec_emb)\n"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Td5Ti-bkalkr"
      },
      "source": [
        "gru_inference_state_input = tf.keras.Input(shape=(latent_dim,), name='hidden_state_input')\n",
        "\n"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWbL1c0qalks"
      },
      "source": [
        "gru_out, gru_state_out = seq2seq_Model.get_layer('Decoder-GRU')([dec_bn, gru_inference_state_input])"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YO-yacmalkt"
      },
      "source": [
        "# Reconstruct dense layers\n",
        "dec_bn2 = seq2seq_Model.get_layer('Decoder-Batchnorm-2')(gru_out)\n",
        "dense_out = seq2seq_Model.get_layer('Final-Output-Dense')(dec_bn2)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-kHrOSWalku"
      },
      "source": [
        "decoder_model = tf.keras.Model([decoder_inputs, gru_inference_state_input],\n",
        "                          [dense_out, gru_state_out])"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIH6DJTgalku"
      },
      "source": [
        "# we want to save the encoder's embedding before its updated by decoder\n",
        "#   because we can use that as an embedding for other tasks.\n",
        "original_body_encoding = body_encoding"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWR750KCalkv"
      },
      "source": [
        "state_value = np.array(tok2.word_index['_start_']).reshape(1, 1)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8vAmv4falkw",
        "outputId": "20ba7b9e-e618-4964-f02f-c55bc3f21f44"
      },
      "source": [
        "state_value"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1]])"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfRcCuiPalky"
      },
      "source": [
        "decoded_sentence = []\n",
        "stop_condition = False"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7muZlgHpalkz"
      },
      "source": [
        "vocabulary_inv = dict((v, k) for k, v in tok2.word_index.items())\n",
        "#vocabulary_inv[0] = \"<PAD/>\"\n",
        "#vocabulary_inv[1] = \"unknown\""
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Q0bJ8-malkz",
        "outputId": "3c5da194-4379-4bfd-e669-86cb00b61024"
      },
      "source": [
        "vocabulary_inv"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1: '_start_',\n",
              " 2: '_end_',\n",
              " 3: 'great',\n",
              " 4: 'good',\n",
              " 5: 'the',\n",
              " 6: 'coffee',\n",
              " 7: 'best',\n",
              " 8: 'love',\n",
              " 9: 'tea',\n",
              " 10: 'for',\n",
              " 11: 'product',\n",
              " 12: 'and',\n",
              " 13: 'it',\n",
              " 14: 'delicious',\n",
              " 15: 'my',\n",
              " 16: 'a',\n",
              " 17: 'not',\n",
              " 18: 'this',\n",
              " 19: 'taste',\n",
              " 20: 'yummy',\n",
              " 21: 'excellent',\n",
              " 22: 'very',\n",
              " 23: 'dog',\n",
              " 24: 'price',\n",
              " 25: 'flavor',\n",
              " 26: 'i',\n",
              " 27: 'of',\n",
              " 28: 'is',\n",
              " 29: 'to',\n",
              " 30: 'these',\n",
              " 31: 'tasty',\n",
              " 32: 'stuff',\n",
              " 33: 'but',\n",
              " 34: 'ever',\n",
              " 35: 'favorite',\n",
              " 36: 'yum',\n",
              " 37: 'like',\n",
              " 38: 'snack',\n",
              " 39: 'food',\n",
              " 40: 'awesome',\n",
              " 41: 'loves',\n",
              " 42: 'in',\n",
              " 43: 'too',\n",
              " 44: 'chocolate',\n",
              " 45: 'wonderful',\n",
              " 46: 'dogs',\n",
              " 47: 'chips',\n",
              " 48: 'as',\n",
              " 49: 'so',\n",
              " 50: 'them',\n",
              " 51: 'treats',\n",
              " 52: 'just',\n",
              " 53: 'are',\n",
              " 54: 'nice',\n",
              " 55: 'hot',\n",
              " 56: 'healthy',\n",
              " 57: 'free',\n",
              " 58: 'with',\n",
              " 59: 'tasting',\n",
              " 60: 'cookies',\n",
              " 61: 'buy',\n",
              " 62: 'treat',\n",
              " 63: 'quality',\n",
              " 64: 'tastes',\n",
              " 65: 'perfect',\n",
              " 66: 'better',\n",
              " 67: 'you',\n",
              " 68: 'on',\n",
              " 69: 'sweet',\n",
              " 70: 'value',\n",
              " 71: 'at',\n",
              " 72: 'green',\n",
              " 73: 'really',\n",
              " 74: 'what',\n",
              " 75: 'no',\n",
              " 76: 'candy',\n",
              " 77: 'its',\n",
              " 78: 'amazing',\n",
              " 79: 'than',\n",
              " 80: 'mix',\n",
              " 81: 'organic',\n",
              " 82: 'cats',\n",
              " 83: 'deal',\n",
              " 84: 'sugar',\n",
              " 85: 'cup',\n",
              " 86: 'gluten',\n",
              " 87: 'cat',\n",
              " 88: 'one',\n",
              " 89: 'service',\n",
              " 90: 'easy',\n",
              " 91: 'ok',\n",
              " 92: 'popcorn',\n",
              " 93: 'bad',\n",
              " 94: 'salt',\n",
              " 95: 'all',\n",
              " 96: 'little',\n",
              " 97: 'bars',\n",
              " 98: 'decaf',\n",
              " 99: 'kcups',\n",
              " 100: 'me',\n",
              " 101: 'gift',\n",
              " 102: 'fantastic',\n",
              " 103: 'review',\n",
              " 104: 'breakfast',\n",
              " 105: 'happy',\n",
              " 106: 'cereal',\n",
              " 107: 'strong',\n",
              " 108: 'oil',\n",
              " 109: 'kcup',\n",
              " 110: 'sauce',\n",
              " 111: 'fresh',\n",
              " 112: 'dont',\n",
              " 113: 'works',\n",
              " 114: 'from',\n",
              " 115: 'fast',\n",
              " 116: 'k',\n",
              " 117: 'expensive',\n",
              " 118: 'loved',\n",
              " 119: 'vanilla',\n",
              " 120: 'dark',\n",
              " 121: 'much',\n",
              " 122: 'coconut',\n",
              " 123: 'blend',\n",
              " 124: 'bold',\n",
              " 125: 'bar',\n",
              " 126: 'way',\n",
              " 127: 'rice',\n",
              " 128: 'super',\n",
              " 129: 'drink',\n",
              " 130: 'have',\n",
              " 131: 'time',\n",
              " 132: 'they',\n",
              " 133: 'pretty',\n",
              " 134: 'cant',\n",
              " 135: 'jerky',\n",
              " 136: 'your',\n",
              " 137: 'gum',\n",
              " 138: 'soup',\n",
              " 139: 'was',\n",
              " 140: 'more',\n",
              " 141: 'be',\n",
              " 142: 'hard',\n",
              " 143: 'real',\n",
              " 144: 'butter',\n",
              " 145: 'amazon',\n",
              " 146: 'low',\n",
              " 147: 'cups',\n",
              " 148: 'go',\n",
              " 149: 'find',\n",
              " 150: 'natural',\n",
              " 151: 'up',\n",
              " 152: 'pasta',\n",
              " 153: 'chai',\n",
              " 154: 'kids',\n",
              " 155: 'smooth',\n",
              " 156: 'baby',\n",
              " 157: 'syrup',\n",
              " 158: 'our',\n",
              " 159: 'nuts',\n",
              " 160: 'kind',\n",
              " 161: 'only',\n",
              " 162: 'fabulous',\n",
              " 163: 'out',\n",
              " 164: 'addictive',\n",
              " 165: 'do',\n",
              " 166: 'item',\n",
              " 167: 'high',\n",
              " 168: 'protein',\n",
              " 169: 'mountain',\n",
              " 170: 'chicken',\n",
              " 171: 'expected',\n",
              " 172: 'beans',\n",
              " 173: 'wow',\n",
              " 174: 'old',\n",
              " 175: 'big',\n",
              " 176: 'honey',\n",
              " 177: 'water',\n",
              " 178: 'bread',\n",
              " 179: 'quick',\n",
              " 180: 'cocoa',\n",
              " 181: 'greenies',\n",
              " 182: 'pretzels',\n",
              " 183: 'shipping',\n",
              " 184: 'spicy',\n",
              " 185: 'cookie',\n",
              " 186: 'had',\n",
              " 187: 'roast',\n",
              " 188: 'ginger',\n",
              " 189: 'disappointed',\n",
              " 190: 'cheaper',\n",
              " 191: 'worth',\n",
              " 192: 'ive',\n",
              " 193: 'that',\n",
              " 194: 'crackers',\n",
              " 195: 'again',\n",
              " 196: 'keurig',\n",
              " 197: 'made',\n",
              " 198: 'get',\n",
              " 199: 'money',\n",
              " 200: 'em',\n",
              " 201: 'milk',\n",
              " 202: 'alternative',\n",
              " 203: 'snacks',\n",
              " 204: 'hazelnut',\n",
              " 205: 'black',\n",
              " 206: 'peanut',\n",
              " 207: 'yuck',\n",
              " 208: 'stale',\n",
              " 209: 'delivery',\n",
              " 210: 'chews',\n",
              " 211: 'stash',\n",
              " 212: 'or',\n",
              " 213: 'overpriced',\n",
              " 214: 'will',\n",
              " 215: 'instant',\n",
              " 216: 'convenient',\n",
              " 217: 'spice',\n",
              " 218: 'seasoning',\n",
              " 219: 'salty',\n",
              " 220: 'there',\n",
              " 221: 'okay',\n",
              " 222: 'wrong',\n",
              " 223: 'fruit',\n",
              " 224: 'by',\n",
              " 225: 'morning',\n",
              " 226: 'energy',\n",
              " 227: 'cheese',\n",
              " 228: 'makes',\n",
              " 229: 'order',\n",
              " 230: 'potato',\n",
              " 231: 'new',\n",
              " 232: 'pods',\n",
              " 233: 'can',\n",
              " 234: 'oatmeal',\n",
              " 235: 'weak',\n",
              " 236: 'well',\n",
              " 237: 'light',\n",
              " 238: 'red',\n",
              " 239: 'bags',\n",
              " 240: 'small',\n",
              " 241: 'beef',\n",
              " 242: 'espresso',\n",
              " 243: 'pop',\n",
              " 244: 'an',\n",
              " 245: 'we',\n",
              " 246: 'horrible',\n",
              " 247: 'if',\n",
              " 248: 'white',\n",
              " 249: 'terrible',\n",
              " 250: 'licorice',\n",
              " 251: 'bitter',\n",
              " 252: 'chip',\n",
              " 253: 'right',\n",
              " 254: 'absolutely',\n",
              " 255: 'french',\n",
              " 256: 'eat',\n",
              " 257: 'doesnt',\n",
              " 258: 'lemon',\n",
              " 259: 'refreshing',\n",
              " 260: 'pack',\n",
              " 261: 'powder',\n",
              " 262: 'delish',\n",
              " 263: 'without',\n",
              " 264: 'use',\n",
              " 265: 'work',\n",
              " 266: 'crunchy',\n",
              " 267: 'always',\n",
              " 268: 'starbucks',\n",
              " 269: 'enough',\n",
              " 270: 'nothing',\n",
              " 271: 'simply',\n",
              " 272: 'around',\n",
              " 273: 'pleased',\n",
              " 274: 'dried',\n",
              " 275: 'almonds',\n",
              " 276: 'awful',\n",
              " 277: 'flavored',\n",
              " 278: 'senseo',\n",
              " 279: 'dry',\n",
              " 280: 'never',\n",
              " 281: 'whole',\n",
              " 282: 'didnt',\n",
              " 283: 'cinnamon',\n",
              " 284: 'packaging',\n",
              " 285: 'variety',\n",
              " 286: 'granola',\n",
              " 287: 'choice',\n",
              " 288: 'glutenfree',\n",
              " 289: 'bit',\n",
              " 290: 'fat',\n",
              " 291: 'does',\n",
              " 292: 'brand',\n",
              " 293: 'flavorful',\n",
              " 294: 'lover',\n",
              " 295: 'seeds',\n",
              " 296: 'fun',\n",
              " 297: 'purchase',\n",
              " 298: '5',\n",
              " 299: 'diet',\n",
              " 300: 'house',\n",
              " 301: 'rich',\n",
              " 302: 'juice',\n",
              " 303: 'size',\n",
              " 304: 'has',\n",
              " 305: 'lovers',\n",
              " 306: 'some',\n",
              " 307: 'cake',\n",
              " 308: 'cappuccino',\n",
              " 309: 'were',\n",
              " 310: 'another',\n",
              " 311: 'nasty',\n",
              " 312: 'customer',\n",
              " 313: 'special',\n",
              " 314: 'make',\n",
              " 315: 'outstanding',\n",
              " 316: 'mint',\n",
              " 317: 'blue',\n",
              " 318: 'flavors',\n",
              " 319: 'world',\n",
              " 320: 'did',\n",
              " 321: 'even',\n",
              " 322: 'nom',\n",
              " 323: 'regular',\n",
              " 324: 'smells',\n",
              " 325: 'nut',\n",
              " 326: 'found',\n",
              " 327: 'goodness',\n",
              " 328: 'box',\n",
              " 329: 'iced',\n",
              " 330: 'substitute',\n",
              " 331: 'maple',\n",
              " 332: 'training',\n",
              " 333: 'italian',\n",
              " 334: 'gross',\n",
              " 335: 'pumpkin',\n",
              " 336: 'puppy',\n",
              " 337: 'apple',\n",
              " 338: 'calm',\n",
              " 339: 'tasted',\n",
              " 340: 'sour',\n",
              " 341: 'thing',\n",
              " 342: 'puck',\n",
              " 343: 'market',\n",
              " 344: 'salmon',\n",
              " 345: 'disgusting',\n",
              " 346: 'over',\n",
              " 347: 'stevia',\n",
              " 348: 'pricey',\n",
              " 349: 'fine',\n",
              " 350: 'off',\n",
              " 351: 'raspberry',\n",
              " 352: 'noodles',\n",
              " 353: 'bones',\n",
              " 354: 'heaven',\n",
              " 355: 'teas',\n",
              " 356: 'decent',\n",
              " 357: 'herbal',\n",
              " 358: 'likes',\n",
              " 359: 'corn',\n",
              " 360: 'would',\n",
              " 361: 'far',\n",
              " 362: 'poor',\n",
              " 363: 'pepper',\n",
              " 364: 'why',\n",
              " 365: 'most',\n",
              " 366: 'hit',\n",
              " 367: 'son',\n",
              " 368: 'china',\n",
              " 369: 'crazy',\n",
              " 370: 'oh',\n",
              " 371: 'satisfied',\n",
              " 372: 'full',\n",
              " 373: 'date',\n",
              " 374: 'original',\n",
              " 375: 'jet',\n",
              " 376: 'every',\n",
              " 377: 'wheat',\n",
              " 378: 'pure',\n",
              " 379: 'fuel',\n",
              " 380: 'different',\n",
              " 381: 'bbq',\n",
              " 382: 'mild',\n",
              " 383: 'must',\n",
              " 384: 'ice',\n",
              " 385: 'biscuits',\n",
              " 386: 'long',\n",
              " 387: 'im',\n",
              " 388: 'other',\n",
              " 389: 'yet',\n",
              " 390: '2',\n",
              " 391: 'baking',\n",
              " 392: 'chew',\n",
              " 393: 'olive',\n",
              " 394: 'received',\n",
              " 395: 'terrific',\n",
              " 396: 'jelly',\n",
              " 397: 'dressing',\n",
              " 398: 'canned',\n",
              " 399: 'aroma',\n",
              " 400: 'earl',\n",
              " 401: 'priced',\n",
              " 402: 'orange',\n",
              " 403: 'bag',\n",
              " 404: 'extra',\n",
              " 405: 'flour',\n",
              " 406: 'grove',\n",
              " 407: 'almost',\n",
              " 408: 'magic',\n",
              " 409: 'got',\n",
              " 410: 'peppermint',\n",
              " 411: 'mustard',\n",
              " 412: 'gf',\n",
              " 413: 'less',\n",
              " 414: 'plus',\n",
              " 415: 'soft',\n",
              " 416: 'grey',\n",
              " 417: 'english',\n",
              " 418: 'exactly',\n",
              " 419: 'when',\n",
              " 420: 'cream',\n",
              " 421: 'wolfgang',\n",
              " 422: 'store',\n",
              " 423: 'finally',\n",
              " 424: 'still',\n",
              " 425: 'newmans',\n",
              " 426: 'crunch',\n",
              " 427: 'vinegar',\n",
              " 428: 'life',\n",
              " 429: 'chili',\n",
              " 430: 'premium',\n",
              " 431: 'here',\n",
              " 432: 'family',\n",
              " 433: 'popchips',\n",
              " 434: 'soy',\n",
              " 435: 'ordered',\n",
              " 436: 'products',\n",
              " 437: 'doggie',\n",
              " 438: 'liked',\n",
              " 439: 'try',\n",
              " 440: 'says',\n",
              " 441: 'mini',\n",
              " 442: 'wellness',\n",
              " 443: 'brew',\n",
              " 444: 'broken',\n",
              " 445: 'last',\n",
              " 446: 'day',\n",
              " 447: 'formula',\n",
              " 448: 'meal',\n",
              " 449: 'kitty',\n",
              " 450: 'package',\n",
              " 451: 'pill',\n",
              " 452: 'rocks',\n",
              " 453: 'say',\n",
              " 454: 'greatest',\n",
              " 455: 'about',\n",
              " 456: 'seed',\n",
              " 457: 'described',\n",
              " 458: '1',\n",
              " 459: 'mom',\n",
              " 460: 'how',\n",
              " 461: 'bland',\n",
              " 462: 'blueberry',\n",
              " 463: 'kona',\n",
              " 464: 'bacon',\n",
              " 465: 'back',\n",
              " 466: 'many',\n",
              " 467: 'picky',\n",
              " 468: 'jeans',\n",
              " 469: 'bean',\n",
              " 470: 'soda',\n",
              " 471: 'worst',\n",
              " 472: 'cheap',\n",
              " 473: 'same',\n",
              " 474: 'disappointing',\n",
              " 475: 'sea',\n",
              " 476: 'waste',\n",
              " 477: 'available',\n",
              " 478: 'garlic',\n",
              " 479: 'chewy',\n",
              " 480: 'lovely',\n",
              " 481: 'top',\n",
              " 482: 'yes',\n",
              " 483: 'kettle',\n",
              " 484: 'sticks',\n",
              " 485: 'first',\n",
              " 486: 'could',\n",
              " 487: 'wont',\n",
              " 488: 'grass',\n",
              " 489: 'stars',\n",
              " 490: 'cute',\n",
              " 491: 'company',\n",
              " 492: 'looking',\n",
              " 493: 'filling',\n",
              " 494: 'raw',\n",
              " 495: 'brown',\n",
              " 496: 'any',\n",
              " 497: 'square',\n",
              " 498: 'tried',\n",
              " 499: 'almond',\n",
              " 500: 'coffe',\n",
              " 501: 'pockets',\n",
              " 502: 'cooking',\n",
              " 503: 'beware',\n",
              " 504: 'extract',\n",
              " 505: 'mmmmm',\n",
              " 506: 'delight',\n",
              " 507: 'superb',\n",
              " 508: 'movie',\n",
              " 509: 'arrived',\n",
              " 510: 'simple',\n",
              " 511: 'caribou',\n",
              " 512: 'highly',\n",
              " 513: 'definitely',\n",
              " 514: 'chocolates',\n",
              " 515: 'cracker',\n",
              " 516: 'bought',\n",
              " 517: 'own',\n",
              " 518: 'helps',\n",
              " 519: 'cold',\n",
              " 520: 'caramel',\n",
              " 521: 'carb',\n",
              " 522: 'eating',\n",
              " 523: 'now',\n",
              " 524: 'gloria',\n",
              " 525: 'grain',\n",
              " 526: 'favorites',\n",
              " 527: 'nestle',\n",
              " 528: 'zukes',\n",
              " 529: 'add',\n",
              " 530: 'creamy',\n",
              " 531: 'gone',\n",
              " 532: 'cherry',\n",
              " 533: 'actually',\n",
              " 534: 'slim',\n",
              " 535: 'jam',\n",
              " 536: 'ingredients',\n",
              " 537: 'tuna',\n",
              " 538: 'nutritious',\n",
              " 539: 'cider',\n",
              " 540: 'theyre',\n",
              " 541: 'true',\n",
              " 542: 'peanuts',\n",
              " 543: 'advertised',\n",
              " 544: 'twinings',\n",
              " 545: 'gummy',\n",
              " 546: 'need',\n",
              " 547: 'home',\n",
              " 548: 'wild',\n",
              " 549: 'timothys',\n",
              " 550: 'used',\n",
              " 551: 'rose',\n",
              " 552: 'lot',\n",
              " 553: 'mints',\n",
              " 554: 'calorie',\n",
              " 555: 'everything',\n",
              " 556: '100',\n",
              " 557: 'yummm',\n",
              " 558: 'salad',\n",
              " 559: 'vegan',\n",
              " 560: 'after',\n",
              " 561: 'rip',\n",
              " 562: 'large',\n",
              " 563: 'think',\n",
              " 564: 'toy',\n",
              " 565: 'sodium',\n",
              " 566: 'wine',\n",
              " 567: 'totally',\n",
              " 568: 'chamomile',\n",
              " 569: 'truffle',\n",
              " 570: 'pet',\n",
              " 571: 'gourmet',\n",
              " 572: 'wife',\n",
              " 573: 'bargain',\n",
              " 574: 'bears',\n",
              " 575: 'cherries',\n",
              " 576: 'idea',\n",
              " 577: 'down',\n",
              " 578: 'diamond',\n",
              " 579: 'lipton',\n",
              " 580: 'earth',\n",
              " 581: 'meh',\n",
              " 582: 'lots',\n",
              " 583: 'anywhere',\n",
              " 584: 'teeth',\n",
              " 585: 'yumm',\n",
              " 586: 'strawberry',\n",
              " 587: 'berry',\n",
              " 588: 'name',\n",
              " 589: 'tiny',\n",
              " 590: 'mocha',\n",
              " 591: 'please',\n",
              " 592: 'recommend',\n",
              " 593: 'quite',\n",
              " 594: 'option',\n",
              " 595: 'mmm',\n",
              " 596: 'latte',\n",
              " 597: 'people',\n",
              " 598: 'mmmm',\n",
              " 599: 'fan',\n",
              " 600: 'addicted',\n",
              " 601: 'homemade',\n",
              " 602: 'save',\n",
              " 603: 'bed',\n",
              " 604: 'satisfying',\n",
              " 605: 'wafers',\n",
              " 606: 'robust',\n",
              " 607: 'clean',\n",
              " 608: 'pecan',\n",
              " 609: 'thanks',\n",
              " 610: 'lime',\n",
              " 611: 'stores',\n",
              " 612: 'star',\n",
              " 613: 'close',\n",
              " 614: 'meat',\n",
              " 615: 'crystal',\n",
              " 616: 'fiber',\n",
              " 617: 'power',\n",
              " 618: 'awsome',\n",
              " 619: 'basket',\n",
              " 620: 'average',\n",
              " 621: 'wish',\n",
              " 622: 'pod',\n",
              " 623: 'smell',\n",
              " 624: 'classic',\n",
              " 625: 'bulk',\n",
              " 626: 'hip',\n",
              " 627: 'peach',\n",
              " 628: 'splenda',\n",
              " 629: 'things',\n",
              " 630: 'lavazza',\n",
              " 631: 'seller',\n",
              " 632: 'fix',\n",
              " 633: 'pie',\n",
              " 634: 'addicting',\n",
              " 635: 'medium',\n",
              " 636: 'beautiful',\n",
              " 637: 'toffee',\n",
              " 638: 'joe',\n",
              " 639: 'omg',\n",
              " 640: 'handy',\n",
              " 641: 'husband',\n",
              " 642: 'christmas',\n",
              " 643: 'steak',\n",
              " 644: 'texture',\n",
              " 645: 'aftertaste',\n",
              " 646: '4',\n",
              " 647: 'solid',\n",
              " 648: 'pizza',\n",
              " 649: 'golden',\n",
              " 650: 'should',\n",
              " 651: 'double',\n",
              " 652: 'health',\n",
              " 653: 'yuk',\n",
              " 654: 'paste',\n",
              " 655: 'mango',\n",
              " 656: 'ones',\n",
              " 657: 'dental',\n",
              " 658: 'cans',\n",
              " 659: 'emerils',\n",
              " 660: 'needed',\n",
              " 661: 'tullys',\n",
              " 662: 'hemp',\n",
              " 663: 'who',\n",
              " 664: 'enjoy',\n",
              " 665: 'banana',\n",
              " 666: 'live',\n",
              " 667: 'curry',\n",
              " 668: 'calories',\n",
              " 669: 'summer',\n",
              " 670: 'dessert',\n",
              " 671: 'cafe',\n",
              " 672: 'jack',\n",
              " 673: 'wanted',\n",
              " 674: 'thank',\n",
              " 675: 'delightful',\n",
              " 676: 'candies',\n",
              " 677: '3',\n",
              " 678: 'crack',\n",
              " 679: 'oz',\n",
              " 680: 'tomato',\n",
              " 681: 'tough',\n",
              " 682: 'feel',\n",
              " 683: 'party',\n",
              " 684: 'beer',\n",
              " 685: 'misleading',\n",
              " 686: 'creme',\n",
              " 687: 'everyday',\n",
              " 688: 'foods',\n",
              " 689: 'cola',\n",
              " 690: 'relaxing',\n",
              " 691: 'missing',\n",
              " 692: 'daughter',\n",
              " 693: 'olives',\n",
              " 694: 'bone',\n",
              " 695: 'mill',\n",
              " 696: 'start',\n",
              " 697: 'crisp',\n",
              " 698: 'half',\n",
              " 699: 'lunch',\n",
              " 700: 'tree',\n",
              " 701: 'mmmmmm',\n",
              " 702: 'cost',\n",
              " 703: 'ground',\n",
              " 704: 'pork',\n",
              " 705: 'bobs',\n",
              " 706: 'plant',\n",
              " 707: 'slow',\n",
              " 708: 'quaker',\n",
              " 709: 'stop',\n",
              " 710: 'bottle',\n",
              " 711: 'agave',\n",
              " 712: 'noodle',\n",
              " 713: 'country',\n",
              " 714: 'job',\n",
              " 715: 'expiration',\n",
              " 716: 'bite',\n",
              " 717: 'else',\n",
              " 718: 'five',\n",
              " 719: 'replacement',\n",
              " 720: 'puffs',\n",
              " 721: 'goes',\n",
              " 722: 'fish',\n",
              " 723: 'berries',\n",
              " 724: 'jasmine',\n",
              " 725: 'irish',\n",
              " 726: 'liver',\n",
              " 727: 'thought',\n",
              " 728: 'pancake',\n",
              " 729: 'plain',\n",
              " 730: 'shipment',\n",
              " 731: 'pb',\n",
              " 732: 'rawhide',\n",
              " 733: 'gummi',\n",
              " 734: 'flakes',\n",
              " 735: 'us',\n",
              " 736: 'sugarfree',\n",
              " 737: 'oats',\n",
              " 738: 'yumo',\n",
              " 739: 'authentic',\n",
              " 740: 'selection',\n",
              " 741: 'excelent',\n",
              " 742: 'give',\n",
              " 743: 'grocery',\n",
              " 744: 'bran',\n",
              " 745: 'horsetail',\n",
              " 746: 'kidding',\n",
              " 747: 'keeps',\n",
              " 748: 'dinner',\n",
              " 749: 'delicous',\n",
              " 750: 'everyone',\n",
              " 751: 'brownies',\n",
              " 752: 'artificial',\n",
              " 753: 'sweetener',\n",
              " 754: 'pink',\n",
              " 755: 'kid',\n",
              " 756: 'two',\n",
              " 757: 'wake',\n",
              " 758: 'needs',\n",
              " 759: 'rub',\n",
              " 760: 'pops',\n",
              " 761: 'sampler',\n",
              " 762: 'baked',\n",
              " 763: 'impressed',\n",
              " 764: 'where',\n",
              " 765: 'unique',\n",
              " 766: 'recommended',\n",
              " 767: 'assortment',\n",
              " 768: 'roasted',\n",
              " 769: 'source',\n",
              " 770: 'transaction',\n",
              " 771: 'kit',\n",
              " 772: 'gold',\n",
              " 773: 'donut',\n",
              " 774: 'fragrant',\n",
              " 775: 'rock',\n",
              " 776: 'sardines',\n",
              " 777: 'dr',\n",
              " 778: 'excellant',\n",
              " 779: '12',\n",
              " 780: 'sumatra',\n",
              " 781: 'crispy',\n",
              " 782: 'expired',\n",
              " 783: 'pamelas',\n",
              " 784: 'said',\n",
              " 785: 'turkey',\n",
              " 786: 'contains',\n",
              " 787: 'scrumptious',\n",
              " 788: 'yummmmm',\n",
              " 789: 'style',\n",
              " 790: 'naturals',\n",
              " 791: 'lamb',\n",
              " 792: 'nutrition',\n",
              " 793: 'sesame',\n",
              " 794: 'haribo',\n",
              " 795: 'yummmm',\n",
              " 796: 'nutiva',\n",
              " 797: 'disappointment',\n",
              " 798: 'thin',\n",
              " 799: 'flowers',\n",
              " 800: 'wholesome',\n",
              " 801: '10',\n",
              " 802: 'bites',\n",
              " 803: 'nature',\n",
              " 804: 'melted',\n",
              " 805: 'want',\n",
              " 806: 'something',\n",
              " 807: 'moist',\n",
              " 808: 'cet',\n",
              " 809: 'sweetner',\n",
              " 810: 'surprisingly',\n",
              " 811: 'gets',\n",
              " 812: 'tasteless',\n",
              " 813: 'pleasant',\n",
              " 814: 'interesting',\n",
              " 815: 'hawaiian',\n",
              " 816: 'wasnt',\n",
              " 817: 'jim',\n",
              " 818: 'am',\n",
              " 819: 'pb2',\n",
              " 820: 'bay',\n",
              " 821: 'elsewhere',\n",
              " 822: 'rum',\n",
              " 823: 'spices',\n",
              " 824: 'cool',\n",
              " 825: 'before',\n",
              " 826: 'care',\n",
              " 827: 'staple',\n",
              " 828: 'raisins',\n",
              " 829: 'experience',\n",
              " 830: 'lollipops',\n",
              " 831: 'swiss',\n",
              " 832: 'surprise',\n",
              " 833: 'weight',\n",
              " 834: 'waffles',\n",
              " 835: 'gotta',\n",
              " 836: 'coffees',\n",
              " 837: 'powdered',\n",
              " 838: 'buying',\n",
              " 839: 'sooo',\n",
              " 840: 'hour',\n",
              " 841: 'peggy',\n",
              " 842: 'recipe',\n",
              " 843: 'soooo',\n",
              " 844: 'away',\n",
              " 845: 'incredible',\n",
              " 846: 'caffeine',\n",
              " 847: 'altoids',\n",
              " 848: 'fair',\n",
              " 849: 'addition',\n",
              " 850: 'loose',\n",
              " 851: 'miss',\n",
              " 852: 'strips',\n",
              " 853: 'healthier',\n",
              " 854: 'mac',\n",
              " 855: 'hips',\n",
              " 856: 'freeze',\n",
              " 857: 'sent',\n",
              " 858: 'zico',\n",
              " 859: 'vitality',\n",
              " 860: 'taffy',\n",
              " 861: 'o',\n",
              " 862: 'costco',\n",
              " 863: 'pancakes',\n",
              " 864: 'san',\n",
              " 865: 'coco',\n",
              " 866: 'leaf',\n",
              " 867: 'n',\n",
              " 868: 'columbian',\n",
              " 869: 'yogi',\n",
              " 870: 'dissapointed',\n",
              " 871: 'tastey',\n",
              " 872: 'soothing',\n",
              " 873: 'drinks',\n",
              " 874: 'cashew',\n",
              " 875: 'garden',\n",
              " 876: 'via',\n",
              " 877: 'belly',\n",
              " 878: 'tummy',\n",
              " 879: 'color',\n",
              " 880: 'biscotti',\n",
              " 881: 'she',\n",
              " 882: 'island',\n",
              " 883: 'choc',\n",
              " 884: 'thats',\n",
              " 885: 'wouldnt',\n",
              " 886: 'pick',\n",
              " 887: 'smoked',\n",
              " 888: 'whats',\n",
              " 889: 'herb',\n",
              " 890: 'links',\n",
              " 891: 'liquid',\n",
              " 892: 'yogurt',\n",
              " 893: 'veggie',\n",
              " 894: 'spiced',\n",
              " 895: 'kick',\n",
              " 896: 'diabetics',\n",
              " 897: 'mixed',\n",
              " 898: 'tart',\n",
              " 899: 'reasonable',\n",
              " 900: 'mouth',\n",
              " 901: 'beat',\n",
              " 902: 'strength',\n",
              " 903: 'maker',\n",
              " 904: 'packs',\n",
              " 905: 'wheres',\n",
              " 906: 'babies',\n",
              " 907: 'night',\n",
              " 908: 'his',\n",
              " 909: 'planet',\n",
              " 910: 'marinade',\n",
              " 911: 'effective',\n",
              " 912: 'meow',\n",
              " 913: 'spaghetti',\n",
              " 914: 'beverage',\n",
              " 915: 'jims',\n",
              " 916: 'creamer',\n",
              " 917: 'single',\n",
              " 918: 'ingredient',\n",
              " 919: 'exceptional',\n",
              " 920: 'cranberry',\n",
              " 921: 'packets',\n",
              " 922: 'flavoring',\n",
              " 923: 'leaves',\n",
              " 924: 'daily',\n",
              " 925: 'second',\n",
              " 926: 'gummies',\n",
              " 927: 'celestial',\n",
              " 928: 'glutino',\n",
              " 929: 'being',\n",
              " 930: 'watery',\n",
              " 931: 'friend',\n",
              " 932: 'smart',\n",
              " 933: 'short',\n",
              " 934: 'cheddar',\n",
              " 935: 'sunflower',\n",
              " 936: 'prefer',\n",
              " 937: 'breast',\n",
              " 938: 'sleep',\n",
              " 939: 'sure',\n",
              " 940: 'animal',\n",
              " 941: 'salsa',\n",
              " 942: 'oolong',\n",
              " 943: 'yeast',\n",
              " 944: 'cakes',\n",
              " 945: 'tassimo',\n",
              " 946: 'diets',\n",
              " 947: 'packaged',\n",
              " 948: 'crisps',\n",
              " 949: 'keep',\n",
              " 950: 'hands',\n",
              " 951: 'know',\n",
              " 952: 'seems',\n",
              " 953: 'seasonings',\n",
              " 954: 'pieces',\n",
              " 955: 'slightly',\n",
              " 956: 'farms',\n",
              " 957: 'anything',\n",
              " 958: 'came',\n",
              " 959: 'their',\n",
              " 960: 'break',\n",
              " 961: 'increase',\n",
              " 962: 'root',\n",
              " 963: 'bonsai',\n",
              " 964: 'her',\n",
              " 965: 'duck',\n",
              " 966: 'theater',\n",
              " 967: 'economical',\n",
              " 968: 'extremely',\n",
              " 969: 'gel',\n",
              " 970: 'lean',\n",
              " 971: 'problem',\n",
              " 972: 'de',\n",
              " 973: 'delivered',\n",
              " 974: 'hint',\n",
              " 975: 'concentrate',\n",
              " 976: 'pricing',\n",
              " 977: 'java',\n",
              " 978: 'fennel',\n",
              " 979: 'shipped',\n",
              " 980: 'rinds',\n",
              " 981: 'saver',\n",
              " 982: 'truffles',\n",
              " 983: 'making',\n",
              " 984: 'mediocre',\n",
              " 985: 'once',\n",
              " 986: 'pay',\n",
              " 987: 'drawer',\n",
              " 988: 'others',\n",
              " 989: 'packing',\n",
              " 990: 'walmart',\n",
              " 991: 'minty',\n",
              " 992: 'holiday',\n",
              " 993: 'per',\n",
              " 994: 'prices',\n",
              " 995: 'damaged',\n",
              " 996: 'van',\n",
              " 997: 'wait',\n",
              " 998: 'subscribe',\n",
              " 999: 'torani',\n",
              " 1000: 'sick',\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aq1-4Pacalk1",
        "outputId": "5ac163ad-441b-4aa9-ca14-13c420443d7f"
      },
      "source": [
        "while not stop_condition:\n",
        "    #print(1)\n",
        "    preds, st = decoder_model.predict([state_value, body_encoding])\n",
        "\n",
        "    pred_idx = np.argmax(preds[:, :, 2:]) + 2\n",
        "    pred_word_str = vocabulary_inv[pred_idx]\n",
        "    print(pred_word_str)\n",
        "    if pred_word_str == '_end_' or len(decoded_sentence) >= maxlen2:\n",
        "        stop_condition = True\n",
        "        break\n",
        "    decoded_sentence.append(pred_word_str)\n",
        "\n",
        "    # update the decoder for the next word\n",
        "    body_encoding = st\n",
        "    state_value = np.array(pred_idx).reshape(1, 1)\n",
        "    #print(state_value)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "great\n",
            "product\n",
            "_end_\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "4wOr6sicalk2",
        "outputId": "0f393645-dbb2-4031-b22b-853cd75c9bad"
      },
      "source": [
        "train.tail()"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Summary</th>\n",
              "      <th>Text</th>\n",
              "      <th>text_length</th>\n",
              "      <th>summary_length</th>\n",
              "      <th>summary_lower</th>\n",
              "      <th>summary_no_punctuation</th>\n",
              "      <th>text_lower</th>\n",
              "      <th>text_no_punctuation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>568439</th>\n",
              "      <td>a-ok</td>\n",
              "      <td>We need this for a recipe my wife is intereste...</td>\n",
              "      <td>22</td>\n",
              "      <td>0.0</td>\n",
              "      <td>a-ok</td>\n",
              "      <td>_start_ aok _end_</td>\n",
              "      <td>we need this for a recipe my wife is intereste...</td>\n",
              "      <td>we need this for a recipe my wife is intereste...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>568442</th>\n",
              "      <td>Great Cafe Latte</td>\n",
              "      <td>This product is great.  Gives you so much ener...</td>\n",
              "      <td>28</td>\n",
              "      <td>2.0</td>\n",
              "      <td>great cafe latte</td>\n",
              "      <td>_start_ great cafe latte _end_</td>\n",
              "      <td>this product is great.  gives you so much ener...</td>\n",
              "      <td>this product is great  gives you so much energ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>568448</th>\n",
              "      <td>Very large ground spice jars.</td>\n",
              "      <td>My only complaint is that there's so much of i...</td>\n",
              "      <td>28</td>\n",
              "      <td>4.0</td>\n",
              "      <td>very large ground spice jars.</td>\n",
              "      <td>_start_ very large ground spice jars _end_</td>\n",
              "      <td>my only complaint is that there's so much of i...</td>\n",
              "      <td>my only complaint is that theres so much of it...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>568449</th>\n",
              "      <td>Will not do without</td>\n",
              "      <td>Great for sesame chicken..this is a good if no...</td>\n",
              "      <td>25</td>\n",
              "      <td>3.0</td>\n",
              "      <td>will not do without</td>\n",
              "      <td>_start_ will not do without _end_</td>\n",
              "      <td>great for sesame chicken..this is a good if no...</td>\n",
              "      <td>great for sesame chickenthis is a good if not ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>568453</th>\n",
              "      <td>Great Honey</td>\n",
              "      <td>I am very satisfied ,product is as advertised,...</td>\n",
              "      <td>20</td>\n",
              "      <td>1.0</td>\n",
              "      <td>great honey</td>\n",
              "      <td>_start_ great honey _end_</td>\n",
              "      <td>i am very satisfied ,product is as advertised,...</td>\n",
              "      <td>i am very satisfied product is as advertised i...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                              Summary  ...                                text_no_punctuation\n",
              "568439                           a-ok  ...  we need this for a recipe my wife is intereste...\n",
              "568442               Great Cafe Latte  ...  this product is great  gives you so much energ...\n",
              "568448  Very large ground spice jars.  ...  my only complaint is that theres so much of it...\n",
              "568449            Will not do without  ...  great for sesame chickenthis is a good if not ...\n",
              "568453                    Great Honey  ...  i am very satisfied product is as advertised i...\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    }
  ]
}